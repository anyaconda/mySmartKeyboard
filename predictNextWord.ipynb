{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My N-Gram Model\n",
    "Create a smart keyboard that predicts next word based on previous word(s).  \n",
    "\n",
    "Using SKLearn CountVectorizer object to convert text to frequency counts.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 9/2/2018\n",
    "#prev: based on preliminary work with 3 small corpa in predictNextWord_BigramModel.ipynb\n",
    "#    2-gram model with training wheels\n",
    "#    figured out 2-gram model first, according to N-gram model instructions.\n",
    "#    used regex to find all n-grams with prev word == prev_token.\n",
    "\n",
    "\n",
    "#->  for bigrams, only 1 row where first word = prev_token\n",
    "#    for trigrams, 1+ rows where second word = prev_token\n",
    "#    for n-grams, 1+ rows where word before last = prev_token\n",
    "\n",
    "#prev: 2-gram model, no training wheels\n",
    "#    get data, write function, use function\n",
    "\n",
    "#here: testing performance of 2-gram model with bigger text\n",
    "#    all twitter text (175M words), not enuf memory (only 4GB) for news and blogs\n",
    "#    e2e takes about 35 min, function takes about 3 min to run \n",
    "#    obviosly unacceptable performance\n",
    "#    see Learning notes at the end for summary and performance improvement obvious ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re #for regex and pattern matching\n",
    "import matplotlib.pyplot as plt #for drawing plots\n",
    "%matplotlib inline\n",
    "\n",
    "#NLP libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#not used\n",
    "#from collections import Counter #for document-term counting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start clock\n",
    "start_time=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load Data\n",
    "Data source is 3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in  0.3536584814389547\n"
     ]
    }
   ],
   "source": [
    "#get rid of punctuation\n",
    "#  re explanation: \n",
    "#  refer to https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "#  replaces not (^) word characters or spaces with the empty string. \n",
    "#  Be careful though, the \\w matches underscore too usually for example\n",
    "# originally was\n",
    "#   words_news = re.sub(r'[^\\w\\s]','',open('sampleData/en_US.news_small.txt').read().lower())\n",
    "#get rid of numbers too\n",
    "text_twitter_in = re.sub(r'[^\\w\\s]|[\\d]','',open('myData/en_US.twitter.txt').read().lower())\n",
    "##text_news_in = re.sub(r'[^\\w\\s]|[\\d]','',open('myData/en_US.news.txt').read().lower())\n",
    "##text_blogs_in = re.sub(r'[^\\w\\s]|[\\d]','',open('myData/en_US.blogs.txt').read().lower())\n",
    "\n",
    "### how long did it take to read in text?\n",
    "end_time=time.time()\n",
    "print('Loaded data in ', (end_time - start_time)/ 60 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview - notice \\n markers of each new line\n",
    "#text_news_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"text_news = re.sub(r'\\n',' </s> <s> ', text_news_in)\\ntext_news = ' '.join(('<s>', text_news,'</s>'))\\n\\ntext_blogs = re.sub(r'\\n',' </s> <s> ', text_blogs_in)\\ntext_blogs = ' '.join(('<s>', text_blogs,'</s>'))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#capture beg and end of line with special delimeters <s> </s> \n",
    "text_twitter = re.sub(r'\\n','</s> <s> ',text_twitter_in)\n",
    "text_twitter = ' '.join((' <s>', text_twitter,'</s>'))\n",
    "\n",
    "\"\"\"text_news = re.sub(r'\\n',' </s> <s> ', text_news_in)\n",
    "text_news = ' '.join(('<s>', text_news,'</s>'))\n",
    "\n",
    "text_blogs = re.sub(r'\\n',' </s> <s> ', text_blogs_in)\n",
    "text_blogs = ' '.join(('<s>', text_blogs,'</s>'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview\n",
    "#text_twitter #class string\n",
    "#text_news\n",
    "#text_blogs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all words\n",
    "text_all = text_twitter ## + ' ' + text_news + ' ' + text_blogs\n",
    "text_all2 = []\n",
    "text_all2.append(text_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in  5.365266752243042\n"
     ]
    }
   ],
   "source": [
    "### how long did it take to process text?\n",
    "end_time=time.time()\n",
    "print('Processed text in ', (end_time - start_time)/ 60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter words 174263060\n",
      "All words 174263060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'our woman happy attention affection treat her like a queen and sex her like a pornstar</s> <s>  </s>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate counts\n",
    "print ('Twitter words', len(text_twitter))\n",
    "##print ('News words', len(text_news))\n",
    "##print ('News words', len(text_blogs))\n",
    "print ('All words', len(text_all2[0]))\n",
    "\n",
    "text_all[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  From Text to Tokens to Frequency Counts (bigrams)\n",
    "Convert a collection of text documents to a matrix of token counts.  \n",
    "Generate frequency counts for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM type  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Successful vocab - with bigrams:  5960862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ÔΩÅÔΩêÔΩÖ in',\n",
       " 'ÔΩàÔΩÖÔΩÇbut',\n",
       " 'ÔΩàÔΩÖÔΩÇbut its',\n",
       " 'ÔΩè',\n",
       " 'ÔΩè </s>',\n",
       " 'ÔΩèÔΩè',\n",
       " 'ÔΩèÔΩè </s>',\n",
       " 'ÔΩíÔΩîif',\n",
       " 'ÔΩíÔΩîif youre',\n",
       " 'ÔΩ∞„Éé',\n",
       " 'ÔΩ∞„Éé </s>',\n",
       " 'ÔΩøÔæõÔæóÔΩ≤ÔæåÔæûlivebar',\n",
       " 'ÔΩøÔæõÔæóÔΩ≤ÔæåÔæûlivebar uncle',\n",
       " 'Ôæâ',\n",
       " 'Ôæâ now',\n",
       " 'Ôæâ Ôæâ',\n",
       " 'ÔæâÔæâ',\n",
       " 'ÔæâÔæâ </s>',\n",
       " 'ùõë',\n",
       " 'ùõë day']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need unigrams and bigrams \n",
    "n=2\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)[\\<[\\/]*]?\\b\\w+\\b[\\>*]?', ngram_range=(1, n))\n",
    "#dtm with 1 document\n",
    "dtm = vectorizer.fit_transform(text_all2) #class 'scipy.sparse.csr.csr_matrix'\n",
    "print ('DTM type ', type(dtm))\n",
    "\n",
    "vocab_list = vectorizer.get_feature_names() #class list\n",
    "print('Successful vocab - with bigrams: ', len(vocab_list))\n",
    "\n",
    "#preview vocab and verify trigrams\n",
    "vocab_list[-20:] #class list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5960862 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5960862 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM type  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Query dtm: how many times an n-gram occurs in the text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2360148"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query dtm \n",
    "print ('DTM type ', type(dtm))\n",
    "ngram_value = '</s> <s>' #'am sam'\n",
    "ngram_idx = vocab_list.index(ngram_value)\n",
    "print ('Query dtm: how many times an n-gram occurs in the text')\n",
    "dtm[0,ngram_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. From sparse matrix into NumPy array  \n",
    "NumPy arrays supports a great variety of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM type before:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "DTM type after <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2360149, 2360148, 2360149, ...,       1,       1,       1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert from current format, sparse matrix, into a normal numpy array \n",
    "print ('DTM type before: ', type(dtm))\n",
    "dtm = dtm.toarray()\n",
    "print ('DTM type after', type(dtm))\n",
    "dtm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['</s>', '</s> <s>', '<s>', '<s> </s>', '<s> _', '<s> __',\n",
       "       '<s> ___', '<s> ____', '<s> _____', '<s> ______'], dtype='<U131')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert python list storing vocab into numpy array\n",
    "vocab = np.array(vocab_list)\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dtm and vocab in  34.95712695121765\n"
     ]
    }
   ],
   "source": [
    "### how long did it take to build?\n",
    "end_time=time.time()\n",
    "print('Built dtm and vocab in ', (end_time - start_time)/ 60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2360148"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query dtm\n",
    "ngram_idx = list(vocab).index(ngram_value)\n",
    "dtm[0,ngram_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NumPy indexing is more natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2360148], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[0,vocab == ngram_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print frequency counts (aka dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2360149, 2360148, 2360149, ...,       1,       1,       1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;/s&gt; &lt;s&gt;</th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>&lt;s&gt; &lt;/s&gt;</th>\n",
       "      <th>&lt;s&gt; _</th>\n",
       "      <th>&lt;s&gt; __</th>\n",
       "      <th>&lt;s&gt; ___</th>\n",
       "      <th>&lt;s&gt; ____</th>\n",
       "      <th>&lt;s&gt; _____</th>\n",
       "      <th>&lt;s&gt; ______</th>\n",
       "      <th>...</th>\n",
       "      <th>ÔΩ∞„Éé &lt;/s&gt;</th>\n",
       "      <th>ÔΩøÔæõÔæóÔΩ≤ÔæåÔæûlivebar</th>\n",
       "      <th>ÔΩøÔæõÔæóÔΩ≤ÔæåÔæûlivebar uncle</th>\n",
       "      <th>Ôæâ</th>\n",
       "      <th>Ôæâ now</th>\n",
       "      <th>Ôæâ Ôæâ</th>\n",
       "      <th>ÔæâÔæâ</th>\n",
       "      <th>ÔæâÔæâ &lt;/s&gt;</th>\n",
       "      <th>ùõë</th>\n",
       "      <th>ùõë day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2360149</td>\n",
       "      <td>2360148</td>\n",
       "      <td>2360149</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>58</td>\n",
       "      <td>42</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 5960862 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      </s>  </s> <s>      <s>  <s> </s>  <s> _  <s> __  <s> ___  <s> ____  \\\n",
       "0  2360149   2360148  2360149         1    200      58       42        22   \n",
       "\n",
       "   <s> _____  <s> ______  ...    ÔΩ∞„Éé </s>  ÔΩøÔæõÔæóÔΩ≤ÔæåÔæûlivebar  ÔΩøÔæõÔæóÔΩ≤ÔæåÔæûlivebar uncle  \\\n",
       "0         12          10  ...          1              1                    1   \n",
       "\n",
       "   Ôæâ  Ôæâ now  Ôæâ Ôæâ  ÔæâÔæâ  ÔæâÔæâ </s>  ùõë  ùõë day  \n",
       "0  2      1    1   1        1  1      1  \n",
       "\n",
       "[1 rows x 5960862 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#friendly print dtm frequency counts\n",
    "df = pd.DataFrame(dtm,columns = vocab)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Function to Predict Next Word\n",
    "Calculate specific bigram probabilities (vectorized and list comprehensions).  \n",
    "\n",
    "Don't need to build a matrix of bigrams to unigram.  \n",
    "Only need to compute a subset of rows:  \n",
    " - for bigrams, only 1 row where first word = prev token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict next word - wrap into a function\n",
    "#must have prev declared variables:\n",
    "#vocab\n",
    "#vocab_list\n",
    "#dtm\n",
    "\n",
    "def predictNextWord(prev_term):\n",
    "    #reminders: prev_term = 'your'\n",
    "    \n",
    "    # get a vector (np.array) where value == prev_term\n",
    "    prev_term_exists = np.where(vocab==prev_term,1,0)\n",
    "    ##print(\"exists? \", prev_term_exists.sum()) #given our vocab can only be 1 (found) or 0 (not found)\n",
    "    \n",
    "    # if prev_term exists, predict next word\n",
    "    if prev_term_exists.sum(): \n",
    "\n",
    "        ## get index of a n-gram\n",
    "        ##prev_term_idx = vocab_list.index(prev_term)\n",
    "        # get count of prev_term\n",
    "        prev_term_count = dtm[0,vocab==prev_term]\n",
    "        \n",
    "        #build regex to find all n-grams with prev word == prev_term.\n",
    "        this_regex = r'\\b' + prev_term + r'\\b \\w+?$'\n",
    "        ##print (this_regex)\n",
    "        r = re.compile(this_regex)\n",
    "\n",
    "        #  get all values from list where values match regex pattern\n",
    "        eligible_ngrams_list = list(filter(r.search, vocab_list))\n",
    "        ##print(\"Eligible ngram list: \", eligible_ngrams_list)\n",
    "        \n",
    "        # get index of each bigram\n",
    "        eligible_ngrams_idx = [i for i, w in enumerate(vocab_list) if re.search(this_regex,w)]\n",
    "        ##print(\"Eligible ngram index: \", eligible_ngrams_idx)\n",
    "        \n",
    "        # compute relative bigram probs \n",
    "        eligible_ngrams_probs = [dtm[0,i]/prev_term_count for i, w in enumerate(vocab_list) if re.search(this_regex,w)]\n",
    "        ##print(\"Eligible ngram probs: \", eligible_ngrams_probs)\n",
    "        \n",
    "        # pick the bigram with the highest prob - only one (if more than one, pick the first one)\n",
    "        best_ngram_idx = np.argmax(eligible_ngrams_probs)\n",
    "        print(\"Best ngram index: \", best_ngram_idx)\n",
    "        best_ngram_value = eligible_ngrams_list[best_ngram_idx]\n",
    "        print(\"Best ngram value: \",best_ngram_value)\n",
    "        next_word = best_ngram_value.split()[n-1]\n",
    "        print(\"Predict next word: \", next_word)\n",
    "        \n",
    "        return next_word\n",
    "    else:\n",
    "        #if prev_term doesn't exist, deal with it later\n",
    "        print (\"No such term found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the function - Bigram only (with training wheels)\n",
    "error handling:  \n",
    "add if unigram exists but no bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram index:  2504\n",
      "Best ngram value:  favorite song\n",
      "Predict next word:  song\n",
      "Function predicted next word in  2.144909930229187\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#check if such word exists\n",
    "test_word ='favorite'\n",
    "##print ('Term exists? ', test_word in vocab_list)\n",
    "\n",
    "predictNextWord(test_word)\n",
    "\n",
    "### how long did it take to run the function?\n",
    "end_time=time.time()\n",
    "print('Function predicted next word in ', (end_time - start_time)/ 60 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to read it\n",
    "Given previous word 'your', get all 'eligible' n-grams:  \n",
    "\n",
    "Correct for bigrams: \n",
    "- Given previous word 'your', get all bigrams starting with prev_token.\n",
    "\n",
    "Correct for trigrams:\n",
    "- Given previus two words 'word_x your', get trigrams with prev_token as the word before last.  \n",
    "\n",
    "Later for 3+ grams: \n",
    "- Given previus n words, get n-grams with prev_token as the word before last.  \n",
    "\n",
    "\n",
    "Once we found all 'eligible' n-grams, compile a maxtrix with counts of relevant frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Notes\n",
    "Performance issues:  \n",
    "Twitter words 174263060  \n",
    "Successful vocab - with bigrams:  5960862  \n",
    "Lines 2360149\t2360148\t2360149  \n",
    "\n",
    "Took 35 min to run e2e.\n",
    "Takes about 2.5 min to predict next word\n",
    "\n",
    "Obvious issues to resolve:\n",
    "- Underscores\n",
    "- Unicode characters\n",
    "- Use Counter instead of CountVectorizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
