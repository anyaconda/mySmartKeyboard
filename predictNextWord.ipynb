{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My N-Gram Model\n",
    "Create a smart keyboard that predicts next word based on previous word(s).  \n",
    "\n",
    "Using SKLearn CountVectorizer object to convert text to frequency counts.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 9/2/2018\n",
    "#prev: based on preliminary work with 3 small corpa in predictNextWord_BigramModel.ipynb\n",
    "#    2-gram model with training wheels\n",
    "#    figured out 2-gram model first, according to N-gram model instructions.\n",
    "#    used regex to find all n-grams with prev word == prev_token.\n",
    "\n",
    "\n",
    "#->  for bigrams, only 1 row where first word = prev_token\n",
    "#    for trigrams, 1+ rows where second word = prev_token\n",
    "#    for n-grams, 1+ rows where word before last = prev_token\n",
    "\n",
    "#here: 2-gram model, no training wheels\n",
    "#    get data, write function, use function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re #for regex and pattern matching\n",
    "import matplotlib.pyplot as plt #for drawing plots\n",
    "%matplotlib inline\n",
    "\n",
    "#NLP libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#not used\n",
    "#from collections import Counter #for document-term counting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start clock\n",
    "start_time=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load Data\n",
    "Data source is 3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in  0.0002685666084289551\n"
     ]
    }
   ],
   "source": [
    "#get rid of punctuation\n",
    "#  re explanation: \n",
    "#  refer to https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "#  replaces not (^) word characters or spaces with the empty string. \n",
    "#  Be careful though, the \\w matches underscore too usually for example\n",
    "# originally was\n",
    "#   words_news = re.sub(r'[^\\w\\s]','',open('sampleData/en_US.news_small.txt').read().lower())\n",
    "#get rid of numbers too\n",
    "text_twitter_in = re.sub(r'[^\\w\\s]|[\\d]','',open('sampleData/en_US.twitter_small.txt').read().lower())\n",
    "text_news_in = re.sub(r'[^\\w\\s]|[\\d]','',open('sampleData/en_US.news_small.txt').read().lower())\n",
    "text_blogs_in = re.sub(r'[^\\w\\s]|[\\d]','',open('sampleData/en_US.blogs_small.txt').read().lower())\n",
    "\n",
    "### how long did it take to read in text?\n",
    "end_time=time.time()\n",
    "print('Loaded data in ', (end_time - start_time)/ 60 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he wasnt home alone apparently\\nthe st louis plant had to close it would die of old age workers had been making cars there since the onset of mass automotive production in the s\\nwsus plans quickly became a hot topic on local online sites though most people applauded plans for the new biomedical center many deplored the potential loss of the building\\nthe alaimo group of mount holly was up for a contract last fall to evaluate and suggest improvements to trenton water works but campaign finance records released this week show the two employees donated a total of  to the political action committee pac partners for progress in early june partners for progress reported it gave more than  in both direct and inkind contributions to mayor tony mack in the two weeks leading up to his victory in the mayoral runoff election june \\nand when its often difficult to predict a laws impact legislators should think twice before carrying any bill is it absolutely necessary is it an issue serious enough to merit their attention will it definitely not make the situation worse\\nthere was a certain amount of scoffing going around a few years ago when the nfl decided to move the draft from the weekend to prime time  eventually splitting off the first round to a separate day\\n charlevoix detroit\\nits just another in a long line of failed attempts to subsidize atlantic city said americans for prosperity new jersey director steve lonegan a conservative who lost to christie in the  gop primary the revel casino hit the jackpot here at government expense\\nbut time and again in the report sullivan called on cps to correct problems to improve employee accountability saying for example that measures to keep employees from submitting fraudulent invoices or to block employees from accessing inappropriate websites were not in place'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview - notice \\n markers of each new line\n",
    "text_news_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture beg and end of line with special delimeters <s> </s> \n",
    "text_twitter = re.sub(r'\\n','</s> <s> ',text_twitter_in)\n",
    "text_twitter = ' '.join((' <s>', text_twitter,'</s>'))\n",
    "\n",
    "text_news = re.sub(r'\\n',' </s> <s> ', text_news_in)\n",
    "text_news = ' '.join(('<s>', text_news,'</s>'))\n",
    "\n",
    "text_blogs = re.sub(r'\\n',' </s> <s> ', text_blogs_in)\n",
    "text_blogs = ' '.join(('<s>', text_blogs,'</s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> he wasnt home alone apparently </s> <s> the st louis plant had to close it would die of old age workers had been making cars there since the onset of mass automotive production in the s </s> <s> wsus plans quickly became a hot topic on local online sites though most people applauded plans for the new biomedical center many deplored the potential loss of the building </s> <s> the alaimo group of mount holly was up for a contract last fall to evaluate and suggest improvements to trenton water works but campaign finance records released this week show the two employees donated a total of  to the political action committee pac partners for progress in early june partners for progress reported it gave more than  in both direct and inkind contributions to mayor tony mack in the two weeks leading up to his victory in the mayoral runoff election june  </s> <s> and when its often difficult to predict a laws impact legislators should think twice before carrying any bill is it absolutely necessary is it an issue serious enough to merit their attention will it definitely not make the situation worse </s> <s> there was a certain amount of scoffing going around a few years ago when the nfl decided to move the draft from the weekend to prime time  eventually splitting off the first round to a separate day </s> <s>  charlevoix detroit </s> <s> its just another in a long line of failed attempts to subsidize atlantic city said americans for prosperity new jersey director steve lonegan a conservative who lost to christie in the  gop primary the revel casino hit the jackpot here at government expense </s> <s> but time and again in the report sullivan called on cps to correct problems to improve employee accountability saying for example that measures to keep employees from submitting fraudulent invoices or to block employees from accessing inappropriate websites were not in place </s>'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview\n",
    "#text_twitter #class string\n",
    "text_news\n",
    "#text_blogs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all words\n",
    "text_all = text_twitter + ' ' + text_news + ' ' + text_blogs\n",
    "text_all2 = []\n",
    "text_all2.append(text_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter words 1692\n",
      "News words 1901\n",
      "News words 7649\n",
      "All words 11244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'t than anything but after staring at it for a while and all of us cheering he started to dig in </s>'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate counts\n",
    "print ('Twitter words', len(text_twitter))\n",
    "print ('News words', len(text_news))\n",
    "print ('News words', len(text_blogs))\n",
    "print ('All words', len(text_all2[0]))\n",
    "\n",
    "text_all[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  From Text to Tokens to Frequency Counts (bigrams)\n",
    "Convert a collection of text documents to a matrix of token counts.  \n",
    "Generate frequency counts for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM type  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Successful vocab - with bigrams:  2794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['you mr',\n",
       " 'you need',\n",
       " 'you out',\n",
       " 'you produce',\n",
       " 'you that',\n",
       " 'you to',\n",
       " 'you who',\n",
       " 'you will',\n",
       " 'youll',\n",
       " 'youll know',\n",
       " 'youll smile',\n",
       " 'your',\n",
       " 'your die',\n",
       " 'your eyes',\n",
       " 'your graduation',\n",
       " 'your heart',\n",
       " 'your mailbox',\n",
       " 'your own',\n",
       " 'yourself',\n",
       " 'yourself about']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need unigrams and bigrams \n",
    "n=2\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)[\\<[\\/]*]?\\b\\w+\\b[\\>*]?', ngram_range=(1, n))\n",
    "#dtm with 1 document\n",
    "dtm = vectorizer.fit_transform(text_all2) #class 'scipy.sparse.csr.csr_matrix'\n",
    "print ('DTM type ', type(dtm))\n",
    "\n",
    "vocab_list = vectorizer.get_feature_names() #class list\n",
    "print('Successful vocab - with bigrams: ', len(vocab_list))\n",
    "\n",
    "#preview vocab and verify trigrams\n",
    "vocab_list[-20:] #class list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2794 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2794 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM type  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Query dtm: how many times an n-gram occurs in the text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query dtm \n",
    "print ('DTM type ', type(dtm))\n",
    "ngram_value = '</s> <s>' #'am sam'\n",
    "ngram_idx = vocab_list.index(ngram_value)\n",
    "print ('Query dtm: how many times an n-gram occurs in the text')\n",
    "dtm[0,ngram_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. From sparse matrix into NumPy array  \n",
    "NumPy arrays supports a great variety of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM type before:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "DTM type after <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([56, 55, 56, ...,  3,  1,  1], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert from current format, sparse matrix, into a normal numpy array \n",
    "print ('DTM type before: ', type(dtm))\n",
    "dtm = dtm.toarray()\n",
    "print ('DTM type after', type(dtm))\n",
    "dtm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['</s>', '</s> <s>', '<s>', '<s> a', '<s> although', '<s> and',\n",
       "       '<s> april', '<s> beauty', '<s> but', '<s> chad'], dtype='<U25')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert python list storing vocab into numpy array\n",
    "vocab = np.array(vocab_list)\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query dtm\n",
    "ngram_idx = list(vocab).index(ngram_value)\n",
    "dtm[0,ngram_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NumPy indexing is more natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[0,vocab == ngram_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print frequency counts (aka dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56, 55, 56, ...,  3,  1,  1], dtype=int64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;/s&gt; &lt;s&gt;</th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>&lt;s&gt; a</th>\n",
       "      <th>&lt;s&gt; although</th>\n",
       "      <th>&lt;s&gt; and</th>\n",
       "      <th>&lt;s&gt; april</th>\n",
       "      <th>&lt;s&gt; beauty</th>\n",
       "      <th>&lt;s&gt; but</th>\n",
       "      <th>&lt;s&gt; chad</th>\n",
       "      <th>...</th>\n",
       "      <th>youll smile</th>\n",
       "      <th>your</th>\n",
       "      <th>your die</th>\n",
       "      <th>your eyes</th>\n",
       "      <th>your graduation</th>\n",
       "      <th>your heart</th>\n",
       "      <th>your mailbox</th>\n",
       "      <th>your own</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yourself about</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 2794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   </s>  </s> <s>  <s>  <s> a  <s> although  <s> and  <s> april  <s> beauty  \\\n",
       "0    56        55   56      1             1        1          1           1   \n",
       "\n",
       "   <s> but  <s> chad       ...        youll smile  your  your die  your eyes  \\\n",
       "0        1         1       ...                  1     8         1          1   \n",
       "\n",
       "   your graduation  your heart  your mailbox  your own  yourself  \\\n",
       "0                1           1             1         3         1   \n",
       "\n",
       "   yourself about  \n",
       "0               1  \n",
       "\n",
       "[1 rows x 2794 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#friendly print dtm frequency counts\n",
    "df = pd.DataFrame(dtm,columns = vocab)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Function to Predict Next Word\n",
    "Calculate specific bigram probabilities (vectorized and list comprehensions).  \n",
    "\n",
    "Don't need to build a matrix of bigrams to unigram.  \n",
    "Only need to compute a subset of rows:  \n",
    " - for bigrams, only 1 row where first word = prev token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict next word - wrap into a function\n",
    "#must have prev declared variables:\n",
    "#vocab\n",
    "#vocab_list\n",
    "#dtm\n",
    "\n",
    "def predictNextWord(prev_term):\n",
    "    #reminders: prev_term = 'your'\n",
    "    \n",
    "    # get a vector (np.array) where value == prev_term\n",
    "    prev_term_exists = np.where(vocab==prev_term,1,0)\n",
    "    ##print(\"exists? \", prev_term_exists.sum()) #given our vocab can only be 1 (found) or 0 (not found)\n",
    "    \n",
    "    # if prev_term exists, predict next word\n",
    "    if prev_term_exists.sum(): \n",
    "\n",
    "        ## get index of a n-gram\n",
    "        ##prev_term_idx = vocab_list.index(prev_term)\n",
    "        # get count of prev_term\n",
    "        prev_term_count = dtm[0,vocab==prev_term]\n",
    "        \n",
    "        #build regex to find all n-grams with prev word == prev_term.\n",
    "        this_regex = r'\\b' + prev_term + r'\\b \\w+?$'\n",
    "        ##print (this_regex)\n",
    "        r = re.compile(this_regex)\n",
    "\n",
    "        #  get all values from list where values match regex pattern\n",
    "        eligible_ngrams_list = list(filter(r.search, vocab_list))\n",
    "        print(\"Eligible ngram list: \", eligible_ngrams_list)\n",
    "        \n",
    "        # get index of each bigram\n",
    "        eligible_ngrams_idx = [i for i, w in enumerate(vocab_list) if re.search(this_regex,w)]\n",
    "        print(\"Eligible ngram index: \", eligible_ngrams_idx)\n",
    "        \n",
    "        # compute relative bigram probs \n",
    "        eligible_ngrams_probs = [dtm[0,i]/prev_term_count for i, w in enumerate(vocab_list) if re.search(this_regex,w)]\n",
    "        print(\"Eligible ngram probs: \", eligible_ngrams_probs)\n",
    "        \n",
    "        # pick the bigram with the highest prob - only one (if more than one, pick the first one)\n",
    "        best_ngram_idx = np.argmax(eligible_ngrams_probs)\n",
    "        print(\"Best ngram index: \", best_ngram_idx)\n",
    "        best_ngram_value = eligible_ngrams_list[best_ngram_idx]\n",
    "        print(\"Best ngram value: \",best_ngram_value)\n",
    "        next_word = best_ngram_value.split()[n-1]\n",
    "        print(\"Predict next word: \", next_word)\n",
    "        \n",
    "        return next_word\n",
    "    else:\n",
    "        #if prev_term doesn't exist, deal with it later\n",
    "        print (\"No such term found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the function - Bigram only (with training wheels)\n",
    "error handling:  \n",
    "add if unigram exists but no bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term exists?  True\n",
      "Eligible ngram list:  ['with and', 'with but', 'with circle', 'with graduation', 'with great', 'with making', 'with my', 'with no', 'with not', 'with plots', 'with promise', 'with tes', 'with the', 'with your']\n",
      "Eligible ngram index:  [2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704]\n",
      "Eligible ngram probs:  [array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.11111111]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.22222222]), array([0.05555556])]\n",
      "Best ngram index:  12\n",
      "Best ngram value:  with the\n",
      "Predict next word:  the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if such word exists\n",
    "test_word ='with'\n",
    "print ('Term exists? ', test_word in vocab_list)\n",
    "\n",
    "predictNextWord(test_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to read it\n",
    "Given previous word 'your', get all 'eligible' n-grams:  \n",
    "\n",
    "Correct for bigrams: \n",
    "- Given previous word 'your', get all bigrams starting with prev_token.\n",
    "\n",
    "Correct for trigrams:\n",
    "- Given previus two words 'word_x your', get trigrams with prev_token as the word before last.  \n",
    "\n",
    "Later for 3+ grams: \n",
    "- Given previus n words, get n-grams with prev_token as the word before last.  \n",
    "\n",
    "\n",
    "Once we found all 'eligible' n-grams, compile a maxtrix with counts of relevant frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
