{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My N-Gram Model\n",
    "Create a smart keyboard that predicts next word based on previous word(s).  \n",
    "\n",
    "Using SKLearn CountVectorizer object to convert text to frequency counts.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 8/26/2018\n",
    "#prev: based on preliminary work with 3 small corpa in predictNextWord_CountVectorizer.ipynb\n",
    "\n",
    "#prev: using a small sample of 2 data sources: twitter and news\n",
    "#    got unigrams and bigrams\n",
    "#    no need to pre-compute the matrix unigram x unigram which would give us all probs of a next unigram, given prev word in a bigram\n",
    "#    only calculating prob vector given a start token\n",
    "#    expanded to trigrams - minimal code changes, works with trigrams out of the \"box\"\n",
    "#previously thought only need to look for n-grams starting with prev_token => incorrect\n",
    "#    need to compute a subset of rows:\n",
    "#    for bigrams, only 1 row where first word = prev_token\n",
    "#    for trigrams, 1+ rows where second word = prev_token\n",
    "#    for n-grams, 1+ rows where word before last = prev_token\n",
    "\n",
    "#    using equal, startwith and endswith give only partial results\n",
    "#    need to find a better way to find all n-grams with prev word == prev_token.\n",
    "\n",
    "#prev: use regex to find all n-grams with prev word == prev_token.\n",
    "\n",
    "#here: 2-gram model with training wheels\n",
    "#    figured out 2-gram model first, according to N-gram model instructions.\n",
    "#    used regex to find all n-grams with prev word == prev_token.\n",
    "#    wrote function after \"training wheels\"\n",
    "\n",
    "#next: 2-gram model with no training wheels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re #for regex and pattern matching\n",
    "import matplotlib.pyplot as plt #for drawing plots\n",
    "%matplotlib inline\n",
    "\n",
    "#NLP libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#not used\n",
    "#from collections import Counter #for document-term counting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start clock\n",
    "start_time=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load Data\n",
    "Data source is 3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of punctuation\n",
    "#  re explanation: \n",
    "#  refer to https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "#  replaces not (^) word characters or spaces with the empty string. \n",
    "#  Be careful though, the \\w matches underscore too usually for example\n",
    "# originally was\n",
    "#   words_news = re.sub(r'[^\\w\\s]','',open('sampleData/en_US.news_small.txt').read().lower())\n",
    "#get rid of numbers too\n",
    "text_twitter_in = re.sub(r'[^\\w\\s]|[\\d]','',open('sampleData/en_US.twitter_small.txt').read().lower())\n",
    "text_news_in = re.sub(r'[^\\w\\s]|[\\d]','',open('sampleData/en_US.news_small.txt').read().lower())\n",
    "text_blogs_in = re.sub(r'[^\\w\\s]|[\\d]','',open('sampleData/en_US.blogs_small.txt').read().lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he wasnt home alone apparently\\nthe st louis plant had to close it would die of old age workers had been making cars there since the onset of mass automotive production in the s\\nwsus plans quickly became a hot topic on local online sites though most people applauded plans for the new biomedical center many deplored the potential loss of the building\\nthe alaimo group of mount holly was up for a contract last fall to evaluate and suggest improvements to trenton water works but campaign finance records released this week show the two employees donated a total of  to the political action committee pac partners for progress in early june partners for progress reported it gave more than  in both direct and inkind contributions to mayor tony mack in the two weeks leading up to his victory in the mayoral runoff election june \\nand when its often difficult to predict a laws impact legislators should think twice before carrying any bill is it absolutely necessary is it an issue serious enough to merit their attention will it definitely not make the situation worse\\nthere was a certain amount of scoffing going around a few years ago when the nfl decided to move the draft from the weekend to prime time  eventually splitting off the first round to a separate day\\n charlevoix detroit\\nits just another in a long line of failed attempts to subsidize atlantic city said americans for prosperity new jersey director steve lonegan a conservative who lost to christie in the  gop primary the revel casino hit the jackpot here at government expense\\nbut time and again in the report sullivan called on cps to correct problems to improve employee accountability saying for example that measures to keep employees from submitting fraudulent invoices or to block employees from accessing inappropriate websites were not in place'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview - notice \\n markers of each new line\n",
    "text_news_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture beg and end of line with special delimeters <s> </s> \n",
    "text_twitter = re.sub(r'\\n','</s> <s> ',text_twitter_in)\n",
    "text_twitter = ' '.join((' <s>', text_twitter,'</s>'))\n",
    "\n",
    "text_news = re.sub(r'\\n',' </s> <s> ', text_news_in)\n",
    "text_news = ' '.join(('<s>', text_news,'</s>'))\n",
    "\n",
    "text_blogs = re.sub(r'\\n',' </s> <s> ', text_blogs_in)\n",
    "text_blogs = ' '.join(('<s>', text_blogs,'</s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <s> how are you btw thanks for the rt you gonna be in dc anytime soon love to see you been way way too long</s> <s> when you meet someone special youll know your heart will beat more rapidly and youll smile for no reason</s> <s> theyve decided its more fun if i dont</s> <s> so tired d played lazer tag  ran a lot d ughh going to sleep like in  minutes </s> <s> words from a complete stranger made my birthday even better </s> <s> first cubs game ever wrigley field is gorgeous this is perfect go cubs go</s> <s> i no i get another day off from skool due to the wonderful snow  and this wakes me updamn thing</s> <s> im coo jus at work hella tired r u ever in cali</s> <s> the new sundrop commercial hehe love at first sight</s> <s> we need to reconnect this week</s> <s> i always wonder how the guys on the auctions shows learned to talk so fast all i hear is djsosnekspqnslanskam</s> <s> dammnnnnn what a catch</s> <s> such a great picture the green shirt totally brings out your eyes</s> <s> desk put together room all set up oh boy oh boy</s> <s> im doing it</s> <s> beauty brainstorming in the alchemy office with and sally walker</s> <s> looking for a new band to blog for the month anyone interested</s> <s> packing for a quick move down the street if only i had some movers</s> <s> ford focus hatchback</s> <s> rt  according to the national retail federation  billion was spent on mothersday last year</s> <s>  the tragedy of life is not that it ends so soon but that we wait so long to begin it  wm lewis</s> <s> more skating come by the check out a movie eat a great dinner and top it off with great times at the ice rink</s> <s> watch your mailbox  </s> <s> tommorows the day </s>'"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview\n",
    "text_twitter #class string\n",
    "#text_news\n",
    "#text_blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all words\n",
    "text_all = text_twitter + ' ' + text_news + ' ' + text_blogs\n",
    "text_all2 = []\n",
    "text_all2.append(text_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter words 1692\n",
      "News words 1901\n",
      "News words 7649\n",
      "All words 11244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'t than anything but after staring at it for a while and all of us cheering he started to dig in </s>'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate counts\n",
    "print ('Twitter words', len(text_twitter))\n",
    "print ('News words', len(text_news))\n",
    "print ('News words', len(text_blogs))\n",
    "print ('All words', len(text_all2[0]))\n",
    "\n",
    "text_all[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful regex for this vocab - unigrams with index\n",
      "{'<s>': 1, 'how': 349, 'are': 48, 'you': 884, 'btw': 109, 'thanks': 759, 'for': 284, 'the': 762, 'rt': 649, 'gonna': 309, 'be': 73, 'in': 368, 'dc': 193, 'anytime': 41, 'soon': 706, 'love': 444, 'to': 782, 'see': 668, 'been': 79, 'way': 836, 'too': 787, 'long': 436, '</s>': 0, 'when': 848, 'meet': 469, 'someone': 705, 'special': 708, 'youll': 885, 'know': 406, 'your': 886, 'heart': 332, 'will': 857, 'beat': 75, 'more': 484, 'rapidly': 614, 'and': 34, 'smile': 701, 'no': 514, 'reason': 620, 'theyve': 770, 'decided': 196, 'its': 385, 'fun': 297, 'if': 355, 'i': 351, 'dont': 221, 'so': 703, 'tired': 781, 'd': 189, 'played': 578, 'lazer': 416, 'tag': 748, 'ran': 613, 'a': 2, 'lot': 442, 'ughh': 810, 'going': 308, 'sleep': 696, 'like': 430, 'minutes': 478, 'words': 866, 'from': 295, 'complete': 164, 'stranger': 726, 'made': 449, 'my': 496, 'birthday': 92, 'even': 248, 'better': 87, 'first': 278, 'cubs': 185, 'game': 298, 'ever': 250, 'wrigley': 875, 'field': 271, 'is': 382, 'gorgeous': 313, 'this': 774, 'perfect': 563, 'go': 306, 'get': 301, 'another': 36, 'day': 192, 'off': 526, 'skool': 693, 'due': 225, 'wonderful': 864, 'snow': 702, 'wakes': 827, 'me': 465, 'updamn': 814, 'thing': 771, 'im': 358, 'coo': 174, 'jus': 394, 'at': 57, 'work': 867, 'hella': 335, 'r': 610, 'u': 809, 'cali': 118, 'new': 510, 'sundrop': 739, 'commercial': 162, 'hehe': 334, 'sight': 684, 'we': 837, 'need': 503, 'reconnect': 623, 'week': 840, 'always': 27, 'wonder': 863, 'guys': 322, 'on': 532, 'auctions': 63, 'shows': 682, 'learned': 419, 'talk': 749, 'fast': 263, 'all': 21, 'hear': 331, 'djsosnekspqnslanskam': 215, 'dammnnnnn': 190, 'what': 846, 'catch': 134, 'such': 735, 'great': 318, 'picture': 568, 'green': 319, 'shirt': 679, 'totally': 792, 'brings': 105, 'out': 546, 'eyes': 255, 'desk': 202, 'put': 605, 'together': 783, 'room': 647, 'set': 675, 'up': 813, 'oh': 529, 'boy': 101, 'doing': 218, 'it': 384, 'beauty': 76, 'brainstorming': 103, 'alchemy': 20, 'office': 527, 'with': 860, 'sally': 656, 'walker': 828, 'looking': 438, 'band': 71, 'blog': 97, 'month': 481, 'anyone': 39, 'interested': 377, 'packing': 552, 'quick': 608, 'move': 488, 'down': 223, 'street': 727, 'only': 536, 'had': 323, 'some': 704, 'movers': 490, 'ford': 285, 'focus': 280, 'hatchback': 328, 'according': 7, 'national': 500, 'retail': 637, 'federation': 266, 'billion': 89, 'was': 832, 'spent': 709, 'mothersday': 486, 'last': 412, 'year': 882, 'tragedy': 796, 'of': 525, 'life': 428, 'not': 515, 'that': 760, 'ends': 239, 'but': 114, 'wait': 825, 'begin': 82, 'wm': 862, 'lewis': 425, 'skating': 690, 'come': 160, 'by': 116, 'check': 148, 'movie': 491, 'eat': 228, 'dinner': 210, 'top': 789, 'times': 780, 'ice': 352, 'rink': 645, 'watch': 834, 'mailbox': 450, 'tommorows': 785, 'he': 330, 'wasnt': 833, 'home': 345, 'alone': 23, 'apparently': 43, 'st': 711, 'louis': 443, 'plant': 574, 'close': 157, 'would': 874, 'die': 207, 'old': 531, 'age': 16, 'workers': 868, 'making': 454, 'cars': 131, 'there': 765, 'since': 686, 'onset': 537, 'mass': 460, 'automotive': 65, 'production': 598, 's': 652, 'wsus': 879, 'plans': 573, 'quickly': 609, 'became': 77, 'hot': 348, 'topic': 790, 'local': 433, 'online': 535, 'sites': 688, 'though': 776, 'most': 485, 'people': 562, 'applauded': 44, 'biomedical': 91, 'center': 136, 'many': 456, 'deplored': 200, 'potential': 585, 'loss': 440, 'building': 110, 'alaimo': 19, 'group': 320, 'mount': 487, 'holly': 344, 'contract': 171, 'fall': 259, 'evaluate': 247, 'suggest': 736, 'improvements': 367, 'trenton': 801, 'water': 835, 'works': 870, 'campaign': 120, 'finance': 274, 'records': 624, 'released': 630, 'show': 681, 'two': 808, 'employees': 236, 'donated': 220, 'total': 791, 'political': 582, 'action': 9, 'committee': 163, 'pac': 551, 'partners': 559, 'progress': 599, 'early': 226, 'june': 393, 'reported': 634, 'gave': 299, 'than': 757, 'both': 100, 'direct': 211, 'inkind': 373, 'contributions': 173, 'mayor': 463, 'tony': 786, 'mack': 448, 'weeks': 842, 'leading': 417, 'his': 341, 'victory': 822, 'mayoral': 464, 'runoff': 650, 'election': 231, 'often': 528, 'difficult': 208, 'predict': 586, 'laws': 415, 'impact': 363, 'legislators': 421, 'should': 680, 'think': 773, 'twice': 806, 'before': 80, 'carrying': 130, 'any': 38, 'bill': 88, 'absolutely': 5, 'necessary': 502, 'an': 33, 'issue': 383, 'serious': 674, 'enough': 241, 'merit': 473, 'their': 763, 'attention': 62, 'definitely': 198, 'make': 452, 'situation': 689, 'worse': 872, 'certain': 137, 'amount': 32, 'scoffing': 664, 'around': 51, 'few': 270, 'years': 883, 'ago': 17, 'nfl': 512, 'draft': 224, 'weekend': 841, 'prime': 592, 'time': 779, 'eventually': 249, 'splitting': 710, 'round': 648, 'separate': 672, 'charlevoix': 145, 'detroit': 204, 'just': 395, 'line': 432, 'failed': 257, 'attempts': 60, 'subsidize': 733, 'atlantic': 58, 'city': 154, 'said': 654, 'americans': 31, 'prosperity': 601, 'jersey': 388, 'director': 212, 'steve': 720, 'lonegan': 435, 'conservative': 168, 'who': 854, 'lost': 441, 'christie': 152, 'gop': 312, 'primary': 591, 'revel': 640, 'casino': 133, 'hit': 342, 'jackpot': 387, 'here': 339, 'government': 315, 'expense': 253, 'again': 15, 'report': 633, 'sullivan': 737, 'called': 119, 'cps': 182, 'correct': 177, 'problems': 595, 'improve': 366, 'employee': 235, 'accountability': 8, 'saying': 660, 'example': 251, 'measures': 467, 'keep': 396, 'submitting': 732, 'fraudulent': 289, 'invoices': 381, 'or': 539, 'block': 96, 'accessing': 6, 'inappropriate': 369, 'websites': 838, 'were': 845, 'place': 571, 'thereafter': 766, 'oil': 530, 'fields': 272, 'platforms': 577, 'named': 497, 'after': 14, 'pagan': 554, 'gods': 307, 'mr': 493, 'brown': 107, 'chad': 138, 'has': 327, 'awesome': 67, 'kids': 399, 'holding': 343, 'fort': 287, 'while': 851, 'later': 413, 'usual': 819, 'have': 329, 'busy': 113, 'playing': 579, 'skylander': 695, 'xbox': 881, 'kyan': 408, 'cashed': 132, 'piggy': 569, 'bank': 72, 'wanted': 830, 'bad': 69, 'used': 817, 'gift': 302, 'card': 127, 'saving': 658, 'money': 480, 'never': 509, 'taps': 750, 'into': 379, 'either': 230, 'him': 340, 'count': 180, 'sure': 744, 'very': 821, 'cute': 188, 'reaction': 615, 'realized': 619, 'did': 206, 'also': 24, 'does': 217, 'good': 310, 'job': 389, 'letting': 423, 'lola': 434, 'feel': 267, 'she': 678, 'her': 337, 'switch': 747, 'characters': 143, 'loves': 445, 'almost': 22, 'as': 53, 'much': 494, 'anyways': 42, 'am': 28, 'share': 677, 'decor': 197, 'inspiration': 376, 'storing': 724, 'folder': 281, 'puter': 606, 'these': 767, 'amazing': 29, 'images': 360, 'stored': 722, 'away': 66, 'ready': 618, 'our': 545, 'graduation': 317, 'season': 666, 'right': 642, 'corner': 175, 'nancy': 499, 'whipped': 852, 'help': 336, 'cards': 128, 'gifts': 303, 'occasion': 524, 'change': 140, 'ones': 534, 'stamped': 713, 'memento': 470, 'tuxedo': 805, 'black': 94, 'cut': 187, 'them': 764, 'circle': 153, 'nestabilities': 508, 'embossed': 233, 'kraft': 407, 'red': 625, 'cardstock': 129, 'tes': 756, 'stars': 715, 'impressions': 365, 'plate': 575, 'which': 850, 'double': 222, 'sided': 683, 'gives': 305, 'fantastic': 261, 'patterns': 561, 'can': 121, 'use': 816, 'plates': 576, 'tutorial': 804, 'taylor': 751, 'created': 183, 'one': 533, 'pass': 560, 'through': 778, 'machine': 447, 'using': 818, 'embossing': 234, 'pad': 553, 'kit': 404, 'do': 216, 'super': 740, 'easy': 227, 'alternative': 25, 'argument': 50, 'lets': 422, 'bear': 74, 'other': 542, 'friends': 293, 'similar': 685, 'stories': 723, 'they': 768, 'treated': 799, 'brusquely': 108, 'laurelwood': 414, 'staff': 712, 'same': 657, 'names': 498, 'coming': 161, 'about': 4, 'halfdozen': 324, 'mine': 477, 'refuse': 627, 'step': 719, 'foot': 282, 'because': 78, 'others': 543, 'theyre': 769, 'telling': 754, 'keeping': 397, 'guess': 321, 'although': 26, 'beloved': 85, 'cantab': 124, 'cant': 123, 'claim': 155, 'international': 378, 'recognition': 621, 'afforded': 13, 'station': 718, 'inn': 374, 'otherwise': 544, 'joints': 390, 'twins': 807, 'separated': 673, 'nothing': 516, 'distance': 214, 'lack': 409, 'pretense': 588, 'imitated': 362, 'approximated': 46, 'ordinariness': 540, 'makes': 453, 'peter': 565, 'schiff': 663, 'hard': 326, 'tell': 753, 'look': 437, 'pretty': 589, 'prices': 590, 'afford': 12, 'buy': 115, 'stuff': 729, 'could': 178, 'far': 262, 'individual': 371, 'liberty': 427, 'blame': 95, 'capitalism': 125, 'freedom': 290, 'impetus': 364, 'regulation': 628, 'disaster': 213, 'rid': 641, 'causing': 135, 'problem': 594, 'whether': 849, 'wrong': 878, 'america': 30, 'pain': 555, 'got': 314, 'deal': 194, 'dealing': 195, 'winter': 858, 'sleeps': 697, 'level': 424, 'turning': 803, 'within': 861, 'contact': 170, 'own': 549, 'innermost': 375, 'being': 84, 'best': 86, 'result': 636, 'answers': 37, 'yourself': 887, 'whatever': 847, 'concerns': 166, 'tremendously': 800, 'creative': 184, 'artists': 52, 'kinds': 402, 'turn': 802, 'seek': 669, 'forms': 286, 'wishes': 859, 'produce': 597, 'medium': 468, 'chosen': 151, 'image': 359, 'balance': 70, 'presented': 587, 'those': 775, 'write': 876, 'plots': 581, 'ideas': 354, 'speak': 707, 'imbalanced': 361, 'person': 564, 'world': 871, 'priscilla': 593, 'designs': 201, 'say': 659, 'remember': 632, 'brought': 106, 'white': 853, 'challenge': 139, 'changes': 141, 'manifest': 455, 'ask': 54, 'environment': 244, 'enroute': 242, 'cornwall': 176, 'months': 482, 'slog': 698, 'sun': 738, 'sea': 665, 'needs': 504, 'tourist': 793, 'town': 794, 'bringing': 104, 'back': 68, 'friday': 291, 'cancel': 122, 'bills': 90, 'retreat': 638, 'moved': 489, 'leeds': 420, 'promise': 600, 'millies': 475, 'went': 844, 'nowhere': 520, 'pure': 603, 'large': 411, 'leaf': 418, 'assam': 55, 'waffling': 824, 'thank': 758, 'want': 829, 'strong': 728, 'dark': 191, 'herby': 338, 'frills': 294, 'goodness': 311, 'sake': 655, 'fruit': 296, 'mixers': 479, 'sweetener': 746, 'why': 856, 'tea': 752, 'mention': 472, 'effect': 229, 'night': 513, 'sky': 694, 'raised': 612, 'ignore': 356, 'lancing': 410, 'parish': 558, 'council': 179, 'lighting': 429, 'including': 370, 'indoor': 372, 'football': 283, 'pitch': 570, 'dome': 219, 'subject': 730, 'planning': 572, 'condition': 167, 'ensure': 243, 'utilised': 820, 'appropriate': 45, 'safeguard': 653, 'skies': 691, 'adjacent': 10, 'np': 521, 'state': 717, 'contracts': 172, 'worth': 873, 'over': 547, 'million': 476, 'ringgit': 644, 'unknowing': 812, 'transformers': 798, 'things': 772, 'protected': 602, 'fairytale': 258, 'astounding': 56, 'support': 741, 'marshals': 458, 'phenomenal': 566, 'rain': 611, 'remaining': 631, 'cheery': 150, 'supportive': 743, 'bunch': 111, 'owing': 548, 'nature': 501, 'course': 181, 'closed': 158, 'roads': 646, 'meant': 466, 'supporters': 742, 'knew': 405, 'area': 49, 'able': 3, 'skip': 692, 'seen': 671, 'attend': 61, 'friend': 292, 'kendras': 398, 'wedding': 839, 'joy': 392, 'end': 237, 'waiting': 826, 'neighbor': 505, 'recommended': 622, 'chasing': 147, 'fireflies': 277, 'charles': 144, 'martin': 459, 'id': 353, 'read': 616, 'anything': 40, 'neighbors': 506, 'favorite': 264, 'author': 64, 'story': 725, 'chase': 146, 'journalist': 391, 'parents': 557, 'adopted': 11, 'unc': 811, 'colorful': 159, 'resident': 635, 'small': 699, 'georgia': 300, 'whose': 855, 'sayings': 661, 'kind': 401, 'memorable': 471, 'character': 142, 'investigating': 380, 'orphan': 541, 'mute': 495, 'pushed': 604, 'car': 126, 'train': 797, 'tracks': 795, 'killed': 400, 'loser': 439, 'custodial': 186, 'parent': 556, 'fallout': 260, 'ellen': 232, 'hopkins': 347, 'p': 550, 'obamacongress': 523, 'etc': 246, 'give': 304, 'likely': 431, 'announces': 35, 'next': 511, 'device': 205, 'detonated': 203, 'populated': 583, 'warning': 831, 'obama': 522, 'et': 245, 'al': 18, 'submit': 731, 'midsized': 474, 'major': 451, 'tens': 755, 'thousands': 777, 'atomic': 59, 'bombs': 98, 'ended': 238, 'wwii': 880, 'april': 47, 'beginnings': 83, 'firstly': 279, 'finally': 273, 'finished': 276, 'exams': 252, 'ill': 357, 'graduating': 316, 'may': 462, 'still': 721, 'feels': 268, 'weird': 843, 'business': 112, 'degree': 199, 'ive': 386, 'working': 869, 'four': 288, 'now': 519, 'process': 596, 'secondly': 667, 'began': 81, 'moving': 492, 'boyfriend': 102, 'thats': 761, 'bit': 93, 'nervewrecking': 507, 'reading': 617, 'books': 99, 'opportunity': 538, 'relax': 629, 'enjoy': 240, 'successfully': 734, 'several': 676, 'hefty': 333, 'novels': 518, 'happy': 325, 'quality': 607, 'picked': 567, 'fellowship': 269, 'ring': 643, 'surprisingly': 745, 'considering': 169, 'fears': 265, 'tolkien': 784, 'writing': 877, 'loving': 446, 'clash': 156, 'kings': 403, 'took': 788, 'liberties': 426, 'concerning': 165, 'plot': 580, 'master': 461, 'margarita': 457, 'russian': 651, 'novel': 517, 'refreshing': 626, 'postpone': 584, 'virginia': 823, 'woolf': 865, 'experience': 254, 'find': 275, 'mood': 483, 'returning': 639, 'hoping': 346, 'howie': 350, 'smash': 700, 'face': 256, 'cake': 117, 'sister': 687, 'seemed': 670, 'scared': 662, 'staring': 714, 'us': 815, 'cheering': 149, 'started': 716, 'dig': 209}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['face',\n",
       " 'cake',\n",
       " 'sister',\n",
       " 'seemed',\n",
       " 'scared',\n",
       " 'staring',\n",
       " 'us',\n",
       " 'cheering',\n",
       " 'started',\n",
       " 'dig']"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get frequency counts\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)[\\<[\\/]*]?\\b\\w+\\b[\\>*]?', ngram_range=(1, 1))\n",
    "#print(type(vectorizer))\n",
    "print('Successful regex for this vocab - unigrams with index')\n",
    "\n",
    "vectorizer.fit(text_all2)\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "unigrams = list(vectorizer.vocabulary_)\n",
    "unigrams[-10:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue with Bigrams\n",
    "Building unigram x unigram matrix results in bigram probabilities.  So need to generate frequency counts for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM type  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Successful vocab - with bigrams:  2794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['you mr',\n",
       " 'you need',\n",
       " 'you out',\n",
       " 'you produce',\n",
       " 'you that',\n",
       " 'you to',\n",
       " 'you who',\n",
       " 'you will',\n",
       " 'youll',\n",
       " 'youll know',\n",
       " 'youll smile',\n",
       " 'your',\n",
       " 'your die',\n",
       " 'your eyes',\n",
       " 'your graduation',\n",
       " 'your heart',\n",
       " 'your mailbox',\n",
       " 'your own',\n",
       " 'yourself',\n",
       " 'yourself about']"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need unigrams and bigrams \n",
    "n=2\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)[\\<[\\/]*]?\\b\\w+\\b[\\>*]?', ngram_range=(1, n))\n",
    "#dtm with 1 document\n",
    "dtm = vectorizer.fit_transform(text_all2) #class 'scipy.sparse.csr.csr_matrix'\n",
    "print ('DTM type ', type(dtm))\n",
    "\n",
    "vocab_list = vectorizer.get_feature_names() #class list\n",
    "vocab_total=len(vocab_list)\n",
    "print('Successful vocab - with bigrams: ', vocab_total)\n",
    "\n",
    "#preview vocab and verify trigrams\n",
    "vocab_list[-20:] #class list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2794 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2794 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM type  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Query dtm: how many times an n-gram occurs in the text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query dtm - only works with 1 row;\n",
    "#if multiple rows, there's no instance of '</s> <s>'\n",
    "print ('DTM type ', type(dtm))\n",
    "ngram_value = '</s> <s>'\n",
    "#ngram_value = 'am sam'\n",
    "ngram_idx = vocab_list.index(ngram_value)\n",
    "print ('Query dtm: how many times an n-gram occurs in the text')\n",
    "dtm[0,ngram_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From sparse matrix into NumPy array  \n",
    "NumPy arrays supports a greater variety of operations than a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM type before:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "DTM type after <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([56, 55, 56, ...,  3,  1,  1], dtype=int64)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert from current format, sparse matrix, into a normal numpy array \n",
    "print ('DTM type before: ', type(dtm))\n",
    "dtm = dtm.toarray()\n",
    "print ('DTM type after', type(dtm))\n",
    "dtm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['</s>', '</s> <s>', '<s>', '<s> a', '<s> although', '<s> and',\n",
       "       '<s> april', '<s> beauty', '<s> but', '<s> chad'], dtype='<U25')"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert python list storing vocab into numpy array\n",
    "vocab = np.array(vocab_list)\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query dtm\n",
    "ngram_idx = list(vocab).index(ngram_value)\n",
    "dtm[0,ngram_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NumPy indexing is more natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55], dtype=int64)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[0,vocab == ngram_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print frequency counts (aka dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56, 55, 56, ...,  3,  1,  1], dtype=int64)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>&lt;/s&gt; &lt;s&gt;</th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>&lt;s&gt; a</th>\n",
       "      <th>&lt;s&gt; although</th>\n",
       "      <th>&lt;s&gt; and</th>\n",
       "      <th>&lt;s&gt; april</th>\n",
       "      <th>&lt;s&gt; beauty</th>\n",
       "      <th>&lt;s&gt; but</th>\n",
       "      <th>&lt;s&gt; chad</th>\n",
       "      <th>...</th>\n",
       "      <th>youll smile</th>\n",
       "      <th>your</th>\n",
       "      <th>your die</th>\n",
       "      <th>your eyes</th>\n",
       "      <th>your graduation</th>\n",
       "      <th>your heart</th>\n",
       "      <th>your mailbox</th>\n",
       "      <th>your own</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yourself about</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 2794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   </s>  </s> <s>  <s>  <s> a  <s> although  <s> and  <s> april  <s> beauty  \\\n",
       "0    56        55   56      1             1        1          1           1   \n",
       "\n",
       "   <s> but  <s> chad       ...        youll smile  your  your die  your eyes  \\\n",
       "0        1         1       ...                  1     8         1          1   \n",
       "\n",
       "   your graduation  your heart  your mailbox  your own  yourself  \\\n",
       "0                1           1             1         3         1   \n",
       "\n",
       "   yourself about  \n",
       "0               1  \n",
       "\n",
       "[1 rows x 2794 columns]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print dtm frequency counts\n",
    "df = pd.DataFrame(dtm,columns = vocab)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate some bigram probabilities from this corpus - manually\n",
    "Didn't think I could at this point.  Thought I had to build unigram to bigram matrix first.  \n",
    "\n",
    "Wrong.  I have everything at this point.  Probably not efficient, but sufficient.  Worry about efficiency after figure out stats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P(I|<s>) = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07142857142857142"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate some bigram probabilities - \n",
    "#P(I|<s>) = ? \n",
    "\n",
    "n_gram_of = '<s> i'\n",
    "n_gram_given = '<s>'\n",
    "p_bigram = dtm[0,list(vocab).index(n_gram_of)] / dtm[0,list(vocab).index(n_gram_given)]\n",
    "\n",
    "p_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03571428571428571"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P(get|I) = ?\n",
    "\n",
    "n_gram_of = 'i get'\n",
    "n_gram_given = 'i'\n",
    "p_bigram = dtm[0,list(vocab).index(n_gram_of)] / dtm[0,list(vocab).index(n_gram_given)]\n",
    "\n",
    "p_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P(no|i)  = ?\n",
    "\n",
    "n_gram_of = 'i no'\n",
    "n_gram_given = 'no'\n",
    "p_bigram = dtm[0,list(vocab).index(n_gram_of)] / dtm[0,list(vocab).index(n_gram_given)]\n",
    "\n",
    "p_bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9821428571428571"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P(<s>|</s>)  = .67\n",
    "\n",
    "n_gram_of = '</s> <s>'\n",
    "n_gram_given = '</s>'\n",
    "p_bigram = dtm[0,list(vocab).index(n_gram_of)] / dtm[0,list(vocab).index(n_gram_given)]\n",
    "\n",
    "p_bigram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate specific bigram probabilities - Loop vs Vectorized\n",
    "Don't need to build a matrix of bigrams to unigram.  \n",
    "Only need to compute a subset of rows:  \n",
    " - for bigrams, only 1 row where first word = prev token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab contains a unigram token?  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2785"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_token = 'your'\n",
    "#Q: given 'your', what are all bigrams and their probabilites?\n",
    "#P([bigrams]|your)  = [p1,p2,..., pn] \n",
    "\n",
    "#work with all vocab\n",
    "#print(vocab)\n",
    "\n",
    "#--check if vocab contains a unigram token and if yes, where\n",
    "print ('vocab contains a unigram token? ', prev_token in vocab) #True\n",
    "#vocab.index(prev_token) #works with list, doesn't work with numpy array\n",
    "list(vocab).index(prev_token) #works with numpy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find n-grams (bigrams n=2)\n",
    "Use regex to find all n-grams with prev word == prev_token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\byour\\b \\w+?$\n"
     ]
    }
   ],
   "source": [
    "#build regex\n",
    "my_regex = r'\\b' + prev_token + r'\\b \\w+?$'\n",
    "print (my_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your die\n",
      "your eyes\n",
      "your graduation\n",
      "your heart\n",
      "your mailbox\n",
      "your own\n"
     ]
    }
   ],
   "source": [
    "#--check if vocab contains n-grams with prev word = prev_token 'your' (includes bigrams starting with prev_token)\n",
    "for term in vocab:\n",
    "    #print (term, prev_token)\n",
    "    #print (term == prev_token)\n",
    "    if re.search(my_regex, term):\n",
    "        print (term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized\n",
    "no looping  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your die', 'your eyes', 'your graduation', 'your heart', 'your mailbox', 'your own']\n"
     ]
    }
   ],
   "source": [
    "r = re.compile(my_regex)\n",
    "eligible_ngrams_list = list(filter(r.search, vocab_list)) # Read Note\n",
    "print(eligible_ngrams_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to read it\n",
    "Given previous word 'your', get all 'eligible' n-grams:  \n",
    "\n",
    "Correct for bigrams: \n",
    "- Given previous word 'your', get all bigrams starting with prev_token.\n",
    "\n",
    "Correct for trigrams:\n",
    "- Given previus two words 'word_x your', get trigrams with prev_token as the word before last.  \n",
    "\n",
    "Later for 3+ grams: \n",
    "- Given previus n words, get n-grams with prev_token as the word before last.  \n",
    "\n",
    "\n",
    "Once we found all 'eligible' n-grams, compile a maxtrix with counts of relevant frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute prob of all bigrams \n",
    "\n",
    "Looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram  your [8]\n",
      "your die\n",
      "index  2786\n",
      "count 1\n",
      "relative prob  [0.125]\n",
      "your eyes\n",
      "index  2787\n",
      "count 1\n",
      "relative prob  [0.125]\n",
      "your graduation\n",
      "index  2788\n",
      "count 1\n",
      "relative prob  [0.125]\n",
      "your heart\n",
      "index  2789\n",
      "count 1\n",
      "relative prob  [0.125]\n",
      "your mailbox\n",
      "index  2790\n",
      "count 1\n",
      "relative prob  [0.125]\n",
      "your own\n",
      "index  2791\n",
      "count 3\n",
      "relative prob  [0.375]\n"
     ]
    }
   ],
   "source": [
    "unigram_count = dtm[0,vocab==prev_token]\n",
    "print('Unigram ', prev_token, unigram_count)\n",
    "\n",
    "for ngram in eligible_ngrams_list:\n",
    "    print(ngram)\n",
    "    #print(np.where(vocab==ngram, 1, -1))\n",
    "    \n",
    "    #index based\n",
    "    ngram_idx = vocab_list.index(ngram)\n",
    "    print('index ', ngram_idx)\n",
    "    ngram_count2  = dtm[0,ngram_idx]\n",
    "    print('count', ngram_count2)\n",
    "\n",
    "    #less code - not index based\n",
    "    #ngram_count  = dtm[0,vocab == ngram]\n",
    "    #print('count', ngram_count)\n",
    "    \n",
    "    #compute prob of each bigram | prev_token\n",
    "    ngram_prob = ngram_count2 / unigram_count\n",
    "    print ('relative prob ', ngram_prob)\n",
    "\n",
    "#eligible_idx = vocab_list.index([eligible_ngrams_list]) #$acerror value error - correct behavior\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized \n",
    "\n",
    "Given prev_token, compute probability of all bigrams with first word == prev_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your die',\n",
       " 'your eyes',\n",
       " 'your graduation',\n",
       " 'your heart',\n",
       " 'your mailbox',\n",
       " 'your own']"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reminders: prev_token = 'your'\n",
    "#  get a vector (np.array) where value == term\n",
    "a = np.where(vocab==prev_token,1,0)\n",
    "a.sum() #given our vocab can only be 1 (found) or 0 (not found)\n",
    "\n",
    "#  get index of a n-gram\n",
    "prev_token_idx = vocab_list.index(prev_token)\n",
    "prev_token_idx\n",
    "\n",
    "#  get all values from list where values match regex pattern\n",
    "eligible_ngrams_list = list(filter(r.search, vocab_list))\n",
    "eligible_ngrams_list\n",
    "#note: this list gives us all bigrams we're interested in to build a 2-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: get index of each bigram, compute relative probs and pick the bigram with the highest prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2786, 2787, 2788, 2789, 2790, 2791]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get index of each bigram\n",
    "eligible_ngrams_idx = [i for i, w in enumerate(vocab_list) if re.search(my_regex,w)]\n",
    "eligible_ngrams_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.125]),\n",
       " array([0.125]),\n",
       " array([0.125]),\n",
       " array([0.125]),\n",
       " array([0.125]),\n",
       " array([0.375])]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute relative bigram probs \n",
    "#[print(i) for i, w in enumerate(vocab_list) if re.search(my_regex,w)]\n",
    "eligible_ngrams_probs = [dtm[0,i]/unigram_count for i, w in enumerate(vocab_list) if re.search(my_regex,w)]\n",
    "eligible_ngrams_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ngram index:  5\n",
      "Best ngram value:  your own\n",
      "Predict next word:  own\n"
     ]
    }
   ],
   "source": [
    "# pick the bigram with the highest prob - only one (if more than one, pick the first one)\n",
    "best_ngram_idx = np.argmax(eligible_ngrams_probs)\n",
    "print(\"Best ngram index: \", best_ngram_idx)\n",
    "best_ngram_value = eligible_ngrams_list[best_ngram_idx]\n",
    "print(\"Best ngram value: \",best_ngram_value)\n",
    "next_word = best_ngram_value.split()[n-1]\n",
    "print(\"Predict next word: \", next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] - if want to consider more than one next predictions, and array has equal probs\n",
    "# pick the bigram(s) with the highest prob - if more than one\n",
    "best_ngrams_value = max(eligible_ngrams_probs)\n",
    "\n",
    "#best_ngrams = np.isin(model_probs, best_ngrams_value)\n",
    "#best_ngrams_idx = np.where(best_ngrams, 1, -1)\n",
    "#best_ngrams_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict next word - wrap into a function\n",
    "#must have prev declared variables:\n",
    "#vocab\n",
    "#vocab_list\n",
    "#dtm\n",
    "\n",
    "def predictNextWord(prev_term):\n",
    "    #reminders: prev_term = 'your'\n",
    "    \n",
    "    # get a vector (np.array) where value == prev_term\n",
    "    prev_term_exists = np.where(vocab==prev_term,1,0)\n",
    "    ##print(\"exists? \", prev_term_exists.sum())\n",
    "    \n",
    "    # if prev_term exists, predict next word\n",
    "    if prev_term_exists.sum(): #given our vocab can only be 1 (found) or 0 (not found)\n",
    "\n",
    "        ## get index of a n-gram\n",
    "        ##prev_term_idx = vocab_list.index(prev_term)\n",
    "        # get count of prev_term\n",
    "        prev_term_count = dtm[0,vocab==prev_term]\n",
    "        \n",
    "        #build regex to find all n-grams with prev word == prev_term.\n",
    "        this_regex = r'\\b' + prev_term + r'\\b \\w+?$'\n",
    "        ##print (this_regex)\n",
    "        r = re.compile(this_regex)\n",
    "\n",
    "        #  get all values from list where values match regex pattern\n",
    "        eligible_ngrams_list = list(filter(r.search, vocab_list))\n",
    "        print(\"Eligible ngram list: \", eligible_ngrams_list)\n",
    "        \n",
    "        # get index of each bigram\n",
    "        eligible_ngrams_idx = [i for i, w in enumerate(vocab_list) if re.search(this_regex,w)]\n",
    "        print(\"Eligible ngram index: \", eligible_ngrams_idx)\n",
    "        \n",
    "        # compute relative bigram probs \n",
    "        eligible_ngrams_probs = [dtm[0,i]/prev_term_count for i, w in enumerate(vocab_list) if re.search(this_regex,w)]\n",
    "        print(\"Eligible ngram probs: \", eligible_ngrams_probs)\n",
    "        \n",
    "        # pick the bigram with the highest prob - only one (if more than one, pick the first one)\n",
    "        best_ngram_idx = np.argmax(eligible_ngrams_probs)\n",
    "        print(\"Best ngram index: \", best_ngram_idx)\n",
    "        best_ngram_value = eligible_ngrams_list[best_ngram_idx]\n",
    "        print(\"Best ngram value: \",best_ngram_value)\n",
    "        next_word = best_ngram_value.split()[n-1]\n",
    "        print(\"Predict next word: \", next_word)\n",
    "        \n",
    "        return next_word\n",
    "    else:\n",
    "        #if prev_term doesn't exist, deal with it later\n",
    "        print (\"No such term found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the function - Bigram only (with training wheels)\n",
    "error handling:  \n",
    "add if unigram exists but no bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term exists?  True\n",
      "Eligible ngram list:  ['with and', 'with but', 'with circle', 'with graduation', 'with great', 'with making', 'with my', 'with no', 'with not', 'with plots', 'with promise', 'with tes', 'with the', 'with your']\n",
      "Eligible ngram index:  [2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704]\n",
      "Eligible ngram probs:  [array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.11111111]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.05555556]), array([0.22222222]), array([0.05555556])]\n",
      "Best ngram index:  12\n",
      "Best ngram value:  with the\n",
      "Predict next word:  the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if such word exists\n",
    "test_word ='with'\n",
    "print ('Term exists? ', test_word in vocab_list)\n",
    "\n",
    "predictNextWord(test_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rough notes\n",
    "discard later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ...,  1,  1,  1])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--check if vocab contains tokens starting with 'your' and if yes, where\n",
    "#no loop, check if vocab contains bigram token(s) starting with prev_token \n",
    "#refer to https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.char.html\n",
    "#https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.core.defchararray.startswith.html#numpy.core.defchararray.startswith\n",
    "np.core.defchararray.endswith(vocab, prev_token) #returns boolean array\n",
    "\n",
    "#np.where(re.search(my_regex, vocab[:]),1,-1)\n",
    "np.where(np.core.defchararray.startswith(vocab,prev_token),1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found n-grams starting with:  your\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['by your',\n",
       " 'in your',\n",
       " 'know your',\n",
       " 'only your',\n",
       " 'out your',\n",
       " 'through your',\n",
       " 'watch your',\n",
       " 'with your',\n",
       " 'your']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Found n-grams starting with: ', prev_token)\n",
    "#bigrams_idx = np.where(np.core.defchararray.endswith(vocab, ' ' + prev_token))\n",
    "bigrams_idx = np.where(np.core.defchararray.endswith(vocab, prev_token ))\n",
    "bigrams_idx[0].tolist()\n",
    "list(vocab[bigrams_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pausing here...\n",
    "Pausing 3-gram model.  want to figure out 2-gram model first, according to N-gram model instructions.\n",
    "\n",
    "From previoues notes: Need to find a better way to find all n-grams with prev word == prev_token.  \n",
    "Then proceed to probability calculations...\n",
    "\n",
    "The rest of this code is from the previous version... will resume once figure out the piece just mentioned.\n",
    "#### How to read it\n",
    "Given previous word 'your', get probabilities of the next word or the next two words.  Max probability wins.\n",
    "\n",
    "Issue: not exactly what's needed.  Change in next take.\n",
    "\n",
    "Correct for bigrams: \n",
    "- Given previous word 'your', get probabilities of the next word.\n",
    "\n",
    "Want for trigrams:\n",
    "- Given previus two words 'word_x your', get probabiliteis of the next word.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams and their probabilities: \n",
      "['by your' 'in your' 'know your' 'only your' 'out your' 'through your'\n",
      " 'watch your' 'with your' 'your']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00035790980672870435,\n",
       " 0.00035790980672870435,\n",
       " 0.00035790980672870435,\n",
       " 0.00035790980672870435,\n",
       " 0.00035790980672870435,\n",
       " 0.00035790980672870435,\n",
       " 0.00035790980672870435,\n",
       " 0.00035790980672870435,\n",
       " 0.002863278453829635]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calc prob of relevant tokens\n",
    "dtm[0, bigrams_idx[0].tolist()]\n",
    "bigrams_prob = dtm[0, bigrams_idx[0].tolist()]/vocab_total\n",
    "\n",
    "print(\"Bigrams and their probabilities: \")\n",
    "print(vocab[bigrams_idx])\n",
    "bigrams_prob.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xtra\n",
    "\n",
    "#### Word Counts with CountVectorizer\n",
    "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "#$xtra - code snippet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "own environment\n",
      "own fairytale\n",
      "own innermost\n",
      "\n",
      "\n",
      "\n",
      "--Together\n",
      "own environment\n",
      "own fairytale\n",
      "own innermost\n"
     ]
    }
   ],
   "source": [
    "#$xtra - regex example with a hard coded word\n",
    "#--check if vocab contains bigrams starting with prev token 'your' \n",
    "for term in vocab:\n",
    "    #print (term, prev_token)\n",
    "    #print (term == prev_token)\n",
    "    if re.search(r'^own \\w+?$', term):\n",
    "        print (term)\n",
    "        \n",
    "#--check if vocab contains n-grams with prev token 'your' except bigrams\n",
    "print ('\\n')\n",
    "for term in vocab:\n",
    "    #print (term, prev_token)\n",
    "    #print (term == prev_token)\n",
    "    if re.search(r' own \\w+?$', term):\n",
    "        #print (term.find(prev_token))\n",
    "        print (term)\n",
    "\n",
    "print ('\\n--Together')        \n",
    "#--(combine the two above) check if vocab contains n-grams with prev token 'your' including bigrams\n",
    "for term in vocab:\n",
    "    #print (term, prev_token)\n",
    "    #print (term == prev_token)\n",
    "    if re.search(r'\\bown\\b \\w+?$', term):\n",
    "        #print (term.find(prev_token))\n",
    "        print (term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['own environment', 'own fairytale', 'own innermost']\n"
     ]
    }
   ],
   "source": [
    "#$xtra - vectorized lookup: regex example with a hard coded word; works with list.  gave up on making it work with np.array\n",
    "r = re.compile(r'\\bown\\b \\w+?$')\n",
    "newlist = list(filter(r.search, vocab_list)) # Read Note\n",
    "print(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
